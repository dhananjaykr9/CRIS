{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRIS ‚Äî Model Training & Evaluation\n",
    "\n",
    "Trains Logistic Regression (baseline), Random Forest, and XGBoost on the\n",
    "SQL-engineered feature matrix. Evaluates with PR-AUC, F1, and Confusion Matrix.\n",
    "\n",
    "**Prerequisites:** Run `python src/data_ingestion.py` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print('XGBoost not installed. Skipping XGBoost model.')\n",
    "\n",
    "from src.preprocessing import prepare_data, FEATURE_COLUMNS\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "MODELS_DIR = Path('..') / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Libraries loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data()\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "X_train_scaled = data['X_train_scaled']\n",
    "X_test_scaled = data['X_test_scaled']\n",
    "scaler = data['scaler']\n",
    "\n",
    "print(f'\\nFeatures: {FEATURE_COLUMNS}')\n",
    "print(f'Train shape: {X_train.shape}')\n",
    "print(f'Test shape:  {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Baseline: Logistic Regression ‚îÄ‚îÄ\n",
    "lr = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "print('‚úÖ Logistic Regression trained.')\n",
    "\n",
    "# ‚îÄ‚îÄ Challenger 1: Random Forest ‚îÄ‚îÄ\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)  # RF doesn't need scaling\n",
    "print('‚úÖ Random Forest trained.')\n",
    "\n",
    "# ‚îÄ‚îÄ Challenger 2: XGBoost ‚îÄ‚îÄ\n",
    "if HAS_XGBOOST:\n",
    "    # Compute scale_pos_weight for class imbalance\n",
    "    n_neg = (y_train == 0).sum()\n",
    "    n_pos = (y_train == 1).sum()\n",
    "    spw = n_neg / n_pos if n_pos > 0 else 1\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        scale_pos_weight=spw,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    print('‚úÖ XGBoost trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name, needs_scaling=False):\n",
    "    \"\"\"Evaluate a model and return metrics.\"\"\"\n",
    "    X_eval = X_test_scaled if needs_scaling else X\n",
    "    \n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    \n",
    "    pr_auc = average_precision_score(y, y_prob)\n",
    "    roc = roc_auc_score(y, y_prob)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    \n",
    "    print(f'\\n{\"‚ïê\"*50}')\n",
    "    print(f'  {model_name}')\n",
    "    print(f'{\"‚ïê\"*50}')\n",
    "    print(f'  PR-AUC:   {pr_auc:.4f}')\n",
    "    print(f'  ROC-AUC:  {roc:.4f}')\n",
    "    print(f'  F1-Score: {f1:.4f}')\n",
    "    print(f'\\n{classification_report(y, y_pred, target_names=[\"Safe\", \"Churned\"])}')\n",
    "    \n",
    "    return {'name': model_name, 'model': model, 'pr_auc': pr_auc, 'roc_auc': roc, 'f1': f1,\n",
    "            'y_pred': y_pred, 'y_prob': y_prob, 'needs_scaling': needs_scaling}\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_model(lr, X_test, y_test, 'Logistic Regression', needs_scaling=True))\n",
    "results.append(evaluate_model(rf, X_test, y_test, 'Random Forest'))\n",
    "if HAS_XGBOOST:\n",
    "    results.append(evaluate_model(xgb, X_test, y_test, 'XGBoost'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Comparison table ‚îÄ‚îÄ\n",
    "comparison = pd.DataFrame([{\n",
    "    'Model': r['name'],\n",
    "    'PR-AUC': r['pr_auc'],\n",
    "    'ROC-AUC': r['roc_auc'],\n",
    "    'F1-Score': r['f1']\n",
    "} for r in results]).set_index('Model')\n",
    "\n",
    "print('\\nüìä Model Comparison:')\n",
    "print(comparison.to_string())\n",
    "\n",
    "# ‚îÄ‚îÄ Bar chart ‚îÄ‚îÄ\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "comparison.plot.bar(ax=ax, rot=0, colormap='viridis')\n",
    "ax.set_title('Model Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1.1)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(results), figsize=(6 * len(results), 5))\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, r in zip(axes, results):\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, r['y_pred'],\n",
    "        display_labels=['Safe', 'Churned'],\n",
    "        cmap='Blues',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(r['name'], fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrices', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "for r, color in zip(results, colors):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, r['y_prob'])\n",
    "    ax.plot(recall, precision, color=color, linewidth=2,\n",
    "            label=f\"{r['name']} (PR-AUC={r['pr_auc']:.3f})\")\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=13)\n",
    "ax.set_ylabel('Precision', fontsize=13)\n",
    "ax.set_title('Precision-Recall Curves', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.set_ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best tree-based model for feature importances\n",
    "best_tree = results[1]  # Random Forest\n",
    "\n",
    "importances = pd.Series(\n",
    "    best_tree['model'].feature_importances_,\n",
    "    index=FEATURE_COLUMNS\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "importances.plot.barh(ax=ax, color='#3498db')\n",
    "ax.set_title(f\"Feature Importances ({best_tree['name']})\",\n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model by PR-AUC\n",
    "best = max(results, key=lambda r: r['pr_auc'])\n",
    "print(f'\\nüèÜ Best model: {best[\"name\"]} (PR-AUC: {best[\"pr_auc\"]:.4f})')\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best['model'], MODELS_DIR / 'best_model.pkl')\n",
    "print(f'‚úÖ Model saved to models/best_model.pkl')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODELS_DIR / 'scaler.pkl')\n",
    "print(f'‚úÖ Scaler saved to models/scaler.pkl')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': best['name'],\n",
    "    'pr_auc': best['pr_auc'],\n",
    "    'roc_auc': best['roc_auc'],\n",
    "    'f1': best['f1'],\n",
    "    'features': FEATURE_COLUMNS,\n",
    "    'needs_scaling': best['needs_scaling']\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(MODELS_DIR / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f'‚úÖ Metadata saved to models/model_metadata.json')\n",
    "\n",
    "print(f'\\nüì¶ All artifacts saved to {MODELS_DIR.resolve()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
